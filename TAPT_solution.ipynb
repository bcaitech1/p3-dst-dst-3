{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python380jvsc74a57bd04a48ac2ebbee9fb58a573a6d63a367a79358f58e623a7bbd03a923bfaab69140",
      "display_name": "Python 3.8.0 64-bit ('.venv': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "TAPT_solution.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb758a5a"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from transformers import (BertTokenizer, \n",
        "                          BertModel,  \n",
        "                          AdamW, \n",
        "                          get_linear_schedule_with_warmup, \n",
        "                          BertConfig)\n",
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
        "from data_utils import (load_dataset, \n",
        "                        get_examples_from_dialogues, \n",
        "                        convert_state_dict, \n",
        "                        DSTInputExample, \n",
        "                        OpenVocabDSTFeature, \n",
        "                        DSTPreprocessor, \n",
        "                        WOSDataset)\n",
        "\n",
        "from inference import inference\n",
        "from evaluation import _evaluation"
      ],
      "id": "eb758a5a",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dc29014",
        "outputId": "8e51576c-a0f0-472a-a4e4-2acf41521010"
      },
      "source": [
        "train_data_file = \"/opt/ml/input/data/train_dataset/train_dials.json\"\n",
        "slot_meta = json.load(open(\"/opt/ml/input/data/train_dataset/slot_meta.json\"))\n",
        "ontology = json.load(open(\"/opt/ml/input/data/train_dataset/ontology.json\"))\n",
        "train_data, dev_data, dev_labels = load_dataset(train_data_file)\n",
        "\n",
        "train_examples = get_examples_from_dialogues(train_data,\n",
        "                                             user_first=False,\n",
        "                                             dialogue_level=False)\n",
        "dev_examples = get_examples_from_dialogues(dev_data,\n",
        "                                           user_first=False,\n",
        "                                           dialogue_level=False)"
      ],
      "id": "1dc29014",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6301/6301 [00:00<00:00, 12863.01it/s]\n",
            "100%|██████████| 699/699 [00:00<00:00, 3342.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cddce0f",
        "outputId": "4d4a4a0e-9f30-4de7-ad37-c1462e97bcb4"
      },
      "source": [
        "print(len(train_examples))\n",
        "print(len(dev_examples))"
      ],
      "id": "3cddce0f",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46231\n5014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee874d10"
      },
      "source": [
        "## TRADE Preprocessor\n",
        "\n",
        "BERT Encoder가 적용된 TRADE의 preprocessor입니다."
      ],
      "id": "ee874d10"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a3af720"
      },
      "source": [
        "class TRADEPreprocessor(DSTPreprocessor):\n",
        "    def __init__(\n",
        "        self,\n",
        "        slot_meta,\n",
        "        src_tokenizer,\n",
        "        trg_tokenizer=None,\n",
        "        ontology=None,\n",
        "        max_seq_length=512,\n",
        "    ):\n",
        "        self.slot_meta = slot_meta\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.trg_tokenizer = trg_tokenizer if trg_tokenizer else src_tokenizer\n",
        "        self.ontology = ontology\n",
        "        self.gating2id = {\"none\": 0, \"dontcare\": 1, \"yes\": 2, \"no\": 3, \"ptr\": 4}\n",
        "        self.id2gating = {v: k for k, v in self.gating2id.items()}\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def _convert_example_to_feature(self, example):\n",
        "        dialogue_context = \" [SEP] \".join(example.context_turns + example.current_turn)\n",
        "\n",
        "        input_id = self.src_tokenizer.encode(dialogue_context, add_special_tokens=False)\n",
        "        max_length = self.max_seq_length - 2\n",
        "        if len(input_id) > max_length:\n",
        "            gap = len(input_id) - max_length\n",
        "            input_id = input_id[gap:]\n",
        "\n",
        "        input_id = (\n",
        "            [self.src_tokenizer.cls_token_id]\n",
        "            + input_id\n",
        "            + [self.src_tokenizer.sep_token_id]\n",
        "        )\n",
        "        segment_id = [0] * len(input_id)\n",
        "\n",
        "        target_ids = []\n",
        "        gating_id = []\n",
        "        if not example.label:\n",
        "            example.label = []\n",
        "\n",
        "        state = convert_state_dict(example.label)\n",
        "        for slot in self.slot_meta:\n",
        "            value = state.get(slot, \"none\")\n",
        "            target_id = self.trg_tokenizer.encode(value, add_special_tokens=False) + [\n",
        "                self.trg_tokenizer.sep_token_id\n",
        "            ]\n",
        "            target_ids.append(target_id)\n",
        "            gating_id.append(self.gating2id.get(value, self.gating2id[\"ptr\"]))\n",
        "        target_ids = self.pad_ids(target_ids, self.trg_tokenizer.pad_token_id)\n",
        "        return OpenVocabDSTFeature(\n",
        "            example.guid, input_id, segment_id, gating_id, target_ids\n",
        "        )\n",
        "\n",
        "    def convert_examples_to_features(self, examples):\n",
        "        return list(map(self._convert_example_to_feature, examples))\n",
        "\n",
        "    def recover_state(self, gate_list, gen_list):\n",
        "        assert len(gate_list) == len(self.slot_meta)\n",
        "        assert len(gen_list) == len(self.slot_meta)\n",
        "\n",
        "        recovered = []\n",
        "        for slot, gate, value in zip(self.slot_meta, gate_list, gen_list):\n",
        "            if self.id2gating[gate] == \"none\":\n",
        "                continue\n",
        "\n",
        "            if self.id2gating[gate] in [\"dontcare\", \"yes\", \"no\"]:\n",
        "                recovered.append(\"%s-%s\" % (slot, self.id2gating[gate]))\n",
        "                continue\n",
        "\n",
        "            token_id_list = []\n",
        "            for id_ in value:\n",
        "                if id_ in self.trg_tokenizer.all_special_ids:\n",
        "                    break\n",
        "\n",
        "                token_id_list.append(id_)\n",
        "            value = self.trg_tokenizer.decode(token_id_list, skip_special_tokens=True)\n",
        "\n",
        "            if value == \"none\":\n",
        "                continue\n",
        "\n",
        "            recovered.append(\"%s-%s\" % (slot, value))\n",
        "        return recovered\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        guids = [b.guid for b in batch]\n",
        "        input_ids = torch.LongTensor(\n",
        "            self.pad_ids([b.input_id for b in batch], self.src_tokenizer.pad_token_id)\n",
        "        )\n",
        "        segment_ids = torch.LongTensor(\n",
        "            self.pad_ids([b.segment_id for b in batch], self.src_tokenizer.pad_token_id)\n",
        "        )\n",
        "        input_masks = input_ids.ne(self.src_tokenizer.pad_token_id)\n",
        "\n",
        "        gating_ids = torch.LongTensor([b.gating_id for b in batch])\n",
        "        target_ids = self.pad_id_of_matrix(\n",
        "            [torch.LongTensor(b.target_ids) for b in batch],\n",
        "            self.trg_tokenizer.pad_token_id,\n",
        "        )\n",
        "        return input_ids, segment_ids, input_masks, gating_ids, target_ids, guids"
      ],
      "id": "6a3af720",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d083031"
      },
      "source": [
        "## Convert_Examples_to_Features"
      ],
      "id": "1d083031"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a534ee7",
        "outputId": "c5d18129-7ec1-4322-e186-3d454efd5cdd"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('kykim/bert-kor-base')\n",
        "processor = TRADEPreprocessor(slot_meta, tokenizer, max_seq_length=512)\n",
        "\n",
        "train_features = processor.convert_examples_to_features(train_examples)\n",
        "dev_features = processor.convert_examples_to_features(dev_examples)"
      ],
      "id": "9a534ee7",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12fe89e6",
        "outputId": "4a25582c-2508-4e60-e48b-14ff88b68af4"
      },
      "source": [
        "print(len(train_features))\n",
        "print(len(dev_features))"
      ],
      "id": "12fe89e6",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46231\n5014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e09aa88"
      },
      "source": [
        "# TRADE"
      ],
      "id": "6e09aa88"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eba9a4a"
      },
      "source": [
        "## Model"
      ],
      "id": "4eba9a4a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cb123f9"
      },
      "source": [
        "class TRADE(nn.Module):\n",
        "    def __init__(self, config, slot_vocab, slot_meta, pad_idx=0):\n",
        "        super(TRADE, self).__init__()\n",
        "        self.config = config\n",
        "        self.slot_meta = slot_meta\n",
        "        if config.model_name_or_path:\n",
        "            self.encoder = BertModel.from_pretrained(config.model_name_or_path)\n",
        "        else:\n",
        "            self.encoder = BertModel(config)\n",
        "            \n",
        "        self.decoder = SlotGenerator(\n",
        "            config.vocab_size,\n",
        "            config.hidden_size,\n",
        "            config.hidden_dropout_prob,\n",
        "            config.n_gate,\n",
        "            None,\n",
        "            pad_idx,\n",
        "        )\n",
        "        \n",
        "        self.decoder.set_slot_idx(slot_vocab)\n",
        "        \n",
        "        self.mlm_head = BertOnlyMLMHead(config)\n",
        "        self.tie_weight()\n",
        "\n",
        "    def tie_weight(self):\n",
        "        self.decoder.embed.weight = self.encoder.embeddings.word_embeddings.weight\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask=None, max_len=10, teacher=None):\n",
        "\n",
        "        encoder_output_group = self.encoder(input_ids=input_ids)\n",
        "        encoder_outputs = encoder_output_group[\"last_hidden_state\"]\n",
        "        pooled_output = encoder_output_group[\"pooler_output\"]\n",
        "        \n",
        "        all_point_outputs, all_gate_outputs = self.decoder(\n",
        "            input_ids, encoder_outputs, pooled_output.unsqueeze(0), attention_mask, max_len, teacher\n",
        "        )\n",
        "\n",
        "        return all_point_outputs, all_gate_outputs\n",
        "    \n",
        "    @staticmethod\n",
        "    def mask_tokens(inputs, tokenizer, config, mlm_probability=0.15):\n",
        "        \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "        labels = inputs.clone()\n",
        "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "        probability_matrix = torch.full(labels.shape, mlm_probability).to(device)\n",
        "        #special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
        "\n",
        "        probability_matrix.masked_fill_(torch.eq(labels, 0), value=0.0)\n",
        "\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).to(device=device, dtype=torch.bool) & masked_indices\n",
        "        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids([\"[MASK]\"])[0]\n",
        "\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).to(device=device, dtype=torch.bool) & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(config.vocab_size, labels.shape, device=device, dtype=torch.long)\n",
        "        inputs[indices_random] = random_words[indices_random].to(device)\n",
        "\n",
        "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "        return inputs, labels\n",
        "    \n",
        "    def forward_pretrain(self, input_ids, tokenizer):\n",
        "        input_ids, labels = self.mask_tokens(input_ids, tokenizer, self.config)\n",
        "        encoder_outputs = self.encoder(input_ids=input_ids)\n",
        "        mlm_logits = self.mlm_head(encoder_outputs[\"last_hidden_state\"])\n",
        "        \n",
        "        return mlm_logits, labels\n",
        "    \n",
        "class SlotGenerator(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size, hidden_size, dropout, n_gate, proj_dim=None, pad_idx=0\n",
        "    ):\n",
        "        super(SlotGenerator, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed = nn.Embedding(\n",
        "            vocab_size, hidden_size, padding_idx=pad_idx\n",
        "        )  # shared with encoder\n",
        "\n",
        "        if proj_dim:\n",
        "            self.proj_layer = nn.Linear(hidden_size, proj_dim, bias=False)\n",
        "        else:\n",
        "            self.proj_layer = None\n",
        "        self.hidden_size = proj_dim if proj_dim else hidden_size\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            self.hidden_size, self.hidden_size, 1, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.n_gate = n_gate\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.w_gen = nn.Linear(self.hidden_size * 3, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.w_gate = nn.Linear(self.hidden_size, n_gate)\n",
        "\n",
        "    def set_slot_idx(self, slot_vocab_idx):\n",
        "        whole = []\n",
        "        max_length = max(map(len, slot_vocab_idx))\n",
        "        for idx in slot_vocab_idx:\n",
        "            if len(idx) < max_length:\n",
        "                gap = max_length - len(idx)\n",
        "                idx.extend([self.pad_idx] * gap)\n",
        "            whole.append(idx)\n",
        "        self.slot_embed_idx = whole  # torch.LongTensor(whole)\n",
        "\n",
        "    def embedding(self, x):\n",
        "        x = self.embed(x)\n",
        "        if self.proj_layer:\n",
        "            x = self.proj_layer(x)\n",
        "        return x\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, encoder_output, hidden, input_masks, max_len, teacher=None\n",
        "    ):\n",
        "        input_masks = input_masks.ne(1)\n",
        "        # J, slot_meta : key : [domain, slot] ex> LongTensor([1,2])\n",
        "        # J,2\n",
        "        batch_size = encoder_output.size(0)\n",
        "        slot = torch.LongTensor(self.slot_embed_idx).to(input_ids.device)  ##\n",
        "        slot_e = torch.sum(self.embedding(slot), 1)  # J,d\n",
        "        J = slot_e.size(0)\n",
        "\n",
        "        all_point_outputs = torch.zeros(batch_size, J, max_len, self.vocab_size).to(\n",
        "            input_ids.device\n",
        "        )\n",
        "        \n",
        "        # Parallel Decoding\n",
        "        w = slot_e.repeat(batch_size, 1).unsqueeze(1)\n",
        "        hidden = hidden.repeat_interleave(J, dim=1)\n",
        "        encoder_output = encoder_output.repeat_interleave(J, dim=0)\n",
        "        input_ids = input_ids.repeat_interleave(J, dim=0)\n",
        "        input_masks = input_masks.repeat_interleave(J, dim=0)\n",
        "        for k in range(max_len):\n",
        "            w = self.dropout(w)\n",
        "            _, hidden = self.gru(w, hidden)  # 1,B,D\n",
        "\n",
        "            # B,T,D * B,D,1 => B,T\n",
        "            attn_e = torch.bmm(encoder_output, hidden.permute(1, 2, 0))  # B,T,1\n",
        "            attn_e = attn_e.squeeze(-1).masked_fill(input_masks, -1e9)\n",
        "            attn_history = F.softmax(attn_e, -1)  # B,T\n",
        "\n",
        "            if self.proj_layer:\n",
        "                hidden_proj = torch.matmul(hidden, self.proj_layer.weight)\n",
        "            else:\n",
        "                hidden_proj = hidden\n",
        "\n",
        "            # B,D * D,V => B,V\n",
        "            attn_v = torch.matmul(\n",
        "                hidden_proj.squeeze(0), self.embed.weight.transpose(0, 1)\n",
        "            )  # B,V\n",
        "            attn_vocab = F.softmax(attn_v, -1)\n",
        "\n",
        "            # B,1,T * B,T,D => B,1,D\n",
        "            context = torch.bmm(attn_history.unsqueeze(1), encoder_output)  # B,1,D\n",
        "            p_gen = self.sigmoid(\n",
        "                self.w_gen(torch.cat([w, hidden.transpose(0, 1), context], -1))\n",
        "            )  # B,1\n",
        "            p_gen = p_gen.squeeze(-1)\n",
        "\n",
        "            p_context_ptr = torch.zeros_like(attn_vocab).to(input_ids.device)\n",
        "            p_context_ptr.scatter_add_(1, input_ids, attn_history)  # copy B,V\n",
        "            p_final = p_gen * attn_vocab + (1 - p_gen) * p_context_ptr  # B,V\n",
        "            _, w_idx = p_final.max(-1)\n",
        "\n",
        "            if teacher is not None:\n",
        "                w = self.embedding(teacher[:, :, k]).transpose(0, 1).reshape(batch_size * J, 1, -1)\n",
        "            else:\n",
        "                w = self.embedding(w_idx).unsqueeze(1)  # B,1,D\n",
        "            if k == 0:\n",
        "                gated_logit = self.w_gate(context.squeeze(1))  # B,3\n",
        "                all_gate_outputs = gated_logit.view(batch_size, J, self.n_gate)\n",
        "            all_point_outputs[:, :, k, :] = p_final.view(batch_size, J, self.vocab_size)\n",
        "\n",
        "        return all_point_outputs, all_gate_outputs"
      ],
      "id": "3cb123f9",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56421870"
      },
      "source": [
        "## 모델 및 데이터 로더 정의"
      ],
      "id": "56421870"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d26f2936",
        "outputId": "c6075510-0092-4386-d877-5164e896711a"
      },
      "source": [
        "slot_vocab = []\n",
        "for slot in slot_meta:\n",
        "    slot_vocab.append(\n",
        "        tokenizer.encode(slot.replace('-', ' '),\n",
        "                         add_special_tokens=False)\n",
        "    )\n",
        "    \n",
        "config = BertConfig.from_pretrained('kykim/bert-kor-base')  ### 수정\n",
        "config.model_name_or_path = 'kykim/bert-kor-base'       ### 수정\n",
        "config.n_gate = len(processor.gating2id)\n",
        "config.proj_dim = None\n",
        "model = TRADE(config, slot_vocab, slot_meta)"
      ],
      "id": "d26f2936",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/workspace/.venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbb6d5e2"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data = WOSDataset(train_features)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_loader = DataLoader(train_data, batch_size=16, sampler=train_sampler, collate_fn=processor.collate_fn)\n",
        "\n",
        "dev_data = WOSDataset(dev_features)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_loader = DataLoader(dev_data, batch_size=8, sampler=dev_sampler, collate_fn=processor.collate_fn)"
      ],
      "id": "cbb6d5e2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b706c4e"
      },
      "source": [
        "## Optimizer & Scheduler 선언"
      ],
      "id": "2b706c4e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8593e4b"
      },
      "source": [
        "n_epochs = 16   ### 수정\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.01,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "t_total = len(train_loader) * n_epochs\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)  ### 수정\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=0.1, num_training_steps=t_total\n",
        ")\n",
        "teacher_forcing = 0.5\n",
        "model.to(device)\n",
        "\n",
        "def masked_cross_entropy_for_value(logits, target, pad_idx=0):\n",
        "    mask = target.ne(pad_idx)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    log_probs_flat = torch.log(logits_flat)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / (mask.sum().float())\n",
        "    return loss\n",
        "\n",
        "loss_fnc_1 = masked_cross_entropy_for_value  # generation\n",
        "loss_fnc_2 = nn.CrossEntropyLoss()  # gating\n",
        "loss_fnc_pretrain = nn.CrossEntropyLoss()  # MLM pretrain"
      ],
      "id": "d8593e4b",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fb73492"
      },
      "source": [
        "## Pretraining"
      ],
      "id": "0fb73492"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aaf8687",
        "outputId": "1599c361-995e-46aa-d7c3-e9c6201f8566",
        "tags": []
      },
      "source": [
        "MLM_PRE = False     ### 수정\n",
        "\n",
        "n_pretrain_epochs = 5\n",
        "\n",
        "def mlm_pretrain(loader, n_epochs):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(loader):\n",
        "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
        "        \n",
        "        logits, labels = model.forward_pretrain(input_ids, tokenizer)\n",
        "        loss = loss_fnc_pretrain(logits.view(-1, config.vocab_size), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print('[%d/%d] [%d/%d] %f' % (epoch + 1, n_epochs, step, len(loader), loss.item()))\n"
      ],
      "id": "8aaf8687",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MLM_PRE:\n",
        "    for epoch in range(n_pretrain_epochs):\n",
        "        mlm_pretrain(train_loader, n_pretrain_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58f9d8dd"
      },
      "source": [
        "## 모델 학습"
      ],
      "id": "58f9d8dd"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.isdir(\"./checkpoint\"):\n",
        "    os.mkdir(\"./checkpoint\")\n",
        "\n",
        "if not os.path.isdir(\"./tensorboard\"):\n",
        "    os.mkdir(\"./tensorboard\")\n",
        "\n",
        "logger = SummaryWriter(\n",
        "    log_dir=\"./tensorboard/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"./checkpoint/model_last.bin\"))    ### 수정\n",
        "\n",
        "# 중간에 끊긴 경우"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23538cd2",
        "outputId": "7fc912e2-41f8-42ee-aced-9e3f0e1e4317",
        "tags": []
      },
      "source": [
        "MLM_DURING = True\n",
        "\n",
        "best_score, best_checkpoint = 0, 0\n",
        "for epoch in range(n_epochs):\n",
        "    batch_loss = []\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
        "        if teacher_forcing > 0.0 and random.random() < teacher_forcing:\n",
        "            tf = target_ids\n",
        "        else:\n",
        "            tf = None\n",
        "        \n",
        "        all_point_outputs, all_gate_outputs = model(input_ids, segment_ids, input_masks, target_ids.size(-1))  # gt - length (generation)\n",
        "        loss_1 = loss_fnc_1(all_point_outputs.contiguous(), target_ids.contiguous().view(-1))\n",
        "        loss_2 = loss_fnc_2(all_gate_outputs.contiguous().view(-1, 5), gating_ids.contiguous().view(-1))\n",
        "        loss = loss_1 + loss_2\n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        if step % 100 == 0:\n",
        "            print('[%d/%d] [%d/%d] %f' % (epoch + 1, n_epochs, step, len(train_loader), loss.item()))\n",
        "            logger.add_scalar(\"Train/loss_sum\", loss.item(), epoch * len(train_loader) + step)\n",
        "            logger.add_scalar(\"Train/gen_loss\", loss_1.item(), epoch * len(train_loader) + step)\n",
        "            logger.add_scalar(\"Train/gate_loss\", loss_2.item(), epoch * len(train_loader) + step)\n",
        "            \n",
        "    if MLM_DURING:\n",
        "        mlm_pretrain(train_loader, n_epochs)\n",
        "                \n",
        "    predictions = inference(model, dev_loader, processor, device)\n",
        "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
        "    for k, v in eval_result.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "    \n",
        "    logger.add_scalar(\"Val/joint_goal_accuracy\", eval_result[\"joint_goal_accuracy\"], epoch)\n",
        "    logger.add_scalar(\"Val/turn_slot_accuracy\", eval_result[\"turn_slot_accuracy\"], epoch)\n",
        "    logger.add_scalar(\"Val/slot_f1\", eval_result[\"turn_slot_f1\"], epoch)\n",
        "\n",
        "    if best_score < eval_result['joint_goal_accuracy']:\n",
        "        print(\"Update Best checkpoint!\")\n",
        "        best_score = eval_result['joint_goal_accuracy']\n",
        "        best_checkpoint = epoch\n",
        "\n",
        "        # 모델 저장\n",
        "        torch.save(model.state_dict(), \"./checkpoint/model_best.bin\")\n",
        "        print(\"Best checkpoint saved at ./checkpoint/model_best.bin\")\n",
        "    \n",
        "    torch.save(model.state_dict(), f\"./checkpoint/model_last.bin\")"
      ],
      "id": "23538cd2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "433977455716586, 'turn_slot_accuracy': 0.9955000894614459, 'turn_slot_f1': 0.9774149087978727}\n",
            "joint_goal_accuracy: 0.8433977455716586\n",
            "turn_slot_accuracy: 0.9955000894614459\n",
            "turn_slot_f1: 0.9774149087978727\n",
            "[23/32] [0/2893] 0.000344\n",
            "[23/32] [100/2893] 0.000429\n",
            "[23/32] [200/2893] 0.000437\n",
            "[23/32] [300/2893] 0.003448\n",
            "[23/32] [400/2893] 0.022281\n",
            "[23/32] [500/2893] 0.000382\n",
            "[23/32] [600/2893] 0.000341\n",
            "[23/32] [700/2893] 0.000361\n",
            "[23/32] [800/2893] 0.000863\n",
            "[23/32] [900/2893] 0.000188\n",
            "[23/32] [1000/2893] 0.000690\n",
            "[23/32] [1100/2893] 0.000760\n",
            "[23/32] [1200/2893] 0.000557\n",
            "[23/32] [1300/2893] 0.001107\n",
            "[23/32] [1400/2893] 0.001570\n",
            "[23/32] [1500/2893] 0.000981\n",
            "[23/32] [1600/2893] 0.003965\n",
            "[23/32] [1700/2893] 0.006393\n",
            "[23/32] [1800/2893] 0.003098\n",
            "[23/32] [1900/2893] 0.001412\n",
            "[23/32] [2000/2893] 0.000185\n",
            "[23/32] [2100/2893] 0.000604\n",
            "[23/32] [2200/2893] 0.000757\n",
            "[23/32] [2300/2893] 0.006285\n",
            "[23/32] [2400/2893] 0.000655\n",
            "[23/32] [2500/2893] 0.000559\n",
            "[23/32] [2600/2893] 0.001232\n",
            "[23/32] [2700/2893] 0.001002\n",
            "[23/32] [2800/2893] 0.000517\n",
            "[23/32] [0/2893] 0.112108\n",
            "[23/32] [100/2893] 0.075703\n",
            "[23/32] [200/2893] 0.255488\n",
            "[23/32] [300/2893] 0.270585\n",
            "[23/32] [400/2893] 0.133470\n",
            "[23/32] [500/2893] 0.157764\n",
            "[23/32] [600/2893] 0.208845\n",
            "[23/32] [700/2893] 0.095001\n",
            "[23/32] [800/2893] 0.052522\n",
            "[23/32] [900/2893] 0.132084\n",
            "[23/32] [1000/2893] 0.168387\n",
            "[23/32] [1100/2893] 0.102445\n",
            "[23/32] [1200/2893] 0.073428\n",
            "[23/32] [1300/2893] 0.119485\n",
            "[23/32] [1400/2893] 0.226949\n",
            "[23/32] [1500/2893] 0.129674\n",
            "[23/32] [1600/2893] 0.067974\n",
            "[23/32] [1700/2893] 0.119086\n",
            "[23/32] [1800/2893] 0.075954\n",
            "[23/32] [1900/2893] 0.122758\n",
            "[23/32] [2000/2893] 0.128692\n",
            "[23/32] [2100/2893] 0.135449\n",
            "[23/32] [2200/2893] 0.108155\n",
            "[23/32] [2300/2893] 0.102638\n",
            "[23/32] [2400/2893] 0.178913\n",
            "[23/32] [2500/2893] 0.119392\n",
            "[23/32] [2600/2893] 0.107712\n",
            "[23/32] [2700/2893] 0.118027\n",
            "[23/32] [2800/2893] 0.188436\n",
            "100%|██████████| 621/621 [01:52<00:00,  5.54it/s]\n",
            "{'joint_goal_accuracy': 0.841988727858293, 'turn_slot_accuracy': 0.9954374664519626, 'turn_slot_f1': 0.9773007733802822}\n",
            "joint_goal_accuracy: 0.841988727858293\n",
            "turn_slot_accuracy: 0.9954374664519626\n",
            "turn_slot_f1: 0.9773007733802822\n",
            "[24/32] [0/2893] 0.002158\n",
            "[24/32] [100/2893] 0.000569\n",
            "[24/32] [200/2893] 0.002147\n",
            "[24/32] [300/2893] 0.000545\n",
            "[24/32] [400/2893] 0.000432\n",
            "[24/32] [500/2893] 0.000447\n",
            "[24/32] [600/2893] 0.000169\n",
            "[24/32] [700/2893] 0.001083\n",
            "[24/32] [800/2893] 0.001283\n",
            "[24/32] [900/2893] 0.003353\n",
            "[24/32] [1000/2893] 0.001350\n",
            "[24/32] [1100/2893] 0.000656\n",
            "[24/32] [1200/2893] 0.002756\n",
            "[24/32] [1300/2893] 0.000628\n",
            "[24/32] [1400/2893] 0.000332\n",
            "[24/32] [1500/2893] 0.000684\n",
            "[24/32] [1600/2893] 0.008130\n",
            "[24/32] [1700/2893] 0.001594\n",
            "[24/32] [1800/2893] 0.000107\n",
            "[24/32] [1900/2893] 0.000451\n",
            "[24/32] [2000/2893] 0.000571\n",
            "[24/32] [2100/2893] 0.008879\n",
            "[24/32] [2200/2893] 0.000528\n",
            "[24/32] [2300/2893] 0.001272\n",
            "[24/32] [2400/2893] 0.004537\n",
            "[24/32] [2500/2893] 0.000727\n",
            "[24/32] [2600/2893] 0.001314\n",
            "[24/32] [2700/2893] 0.000504\n",
            "[24/32] [2800/2893] 0.000834\n",
            "[24/32] [0/2893] 0.156166\n",
            "[24/32] [100/2893] 0.121044\n",
            "[24/32] [200/2893] 0.132874\n",
            "[24/32] [300/2893] 0.109983\n",
            "[24/32] [400/2893] 0.171872\n",
            "[24/32] [500/2893] 0.127755\n",
            "[24/32] [600/2893] 0.142152\n",
            "[24/32] [700/2893] 0.211303\n",
            "[24/32] [800/2893] 0.177846\n",
            "[24/32] [900/2893] 0.139018\n",
            "[24/32] [1000/2893] 0.212777\n",
            "[24/32] [1100/2893] 0.083059\n",
            "[24/32] [1200/2893] 0.184170\n",
            "[24/32] [1300/2893] 0.205248\n",
            "[24/32] [1400/2893] 0.090426\n",
            "[24/32] [1500/2893] 0.184536\n",
            "[24/32] [1600/2893] 0.131316\n",
            "[24/32] [1700/2893] 0.113095\n",
            "[24/32] [1800/2893] 0.147257\n",
            "[24/32] [1900/2893] 0.128930\n",
            "[24/32] [2000/2893] 0.170981\n",
            "[24/32] [2100/2893] 0.065463\n",
            "[24/32] [2200/2893] 0.185531\n",
            "[24/32] [2300/2893] 0.218465\n",
            "[24/32] [2400/2893] 0.173744\n",
            "[24/32] [2500/2893] 0.146129\n",
            "[24/32] [2600/2893] 0.189421\n",
            "[24/32] [2700/2893] 0.112107\n",
            "[24/32] [2800/2893] 0.162372\n",
            "100%|██████████| 621/621 [01:50<00:00,  5.62it/s]\n",
            "{'joint_goal_accuracy': 0.8474235104669887, 'turn_slot_accuracy': 0.9956163893361997, 'turn_slot_f1': 0.9780557263118294}\n",
            "joint_goal_accuracy: 0.8474235104669887\n",
            "turn_slot_accuracy: 0.9956163893361997\n",
            "turn_slot_f1: 0.9780557263118294\n",
            "[25/32] [0/2893] 0.003480\n",
            "[25/32] [100/2893] 0.003699\n",
            "[25/32] [200/2893] 0.000224\n",
            "[25/32] [300/2893] 0.000856\n",
            "[25/32] [400/2893] 0.001193\n",
            "[25/32] [500/2893] 0.002496\n",
            "[25/32] [600/2893] 0.000104\n",
            "[25/32] [700/2893] 0.000325\n",
            "[25/32] [800/2893] 0.000175\n",
            "[25/32] [900/2893] 0.000417\n",
            "[25/32] [1000/2893] 0.000903\n",
            "[25/32] [1100/2893] 0.000583\n",
            "[25/32] [1200/2893] 0.000263\n",
            "[25/32] [1300/2893] 0.000387\n",
            "[25/32] [1400/2893] 0.000660\n",
            "[25/32] [1500/2893] 0.009023\n",
            "[25/32] [1600/2893] 0.002404\n",
            "[25/32] [1700/2893] 0.000466\n",
            "[25/32] [1800/2893] 0.000036\n",
            "[25/32] [1900/2893] 0.000374\n",
            "[25/32] [2000/2893] 0.003227\n",
            "[25/32] [2100/2893] 0.000536\n",
            "[25/32] [2200/2893] 0.000335\n",
            "[25/32] [2300/2893] 0.001943\n",
            "[25/32] [2400/2893] 0.000131\n",
            "[25/32] [2500/2893] 0.000393\n",
            "[25/32] [2600/2893] 0.000341\n",
            "[25/32] [2700/2893] 0.000583\n",
            "[25/32] [2800/2893] 0.000076\n",
            "[25/32] [0/2893] 0.201634\n",
            "[25/32] [100/2893] 0.085427\n",
            "[25/32] [200/2893] 0.167264\n",
            "[25/32] [300/2893] 0.115773\n",
            "[25/32] [400/2893] 0.123754\n",
            "[25/32] [500/2893] 0.166400\n",
            "[25/32] [600/2893] 0.161748\n",
            "[25/32] [700/2893] 0.095396\n",
            "[25/32] [800/2893] 0.105164\n",
            "[25/32] [900/2893] 0.152586\n",
            "[25/32] [1000/2893] 0.086959\n",
            "[25/32] [1100/2893] 0.148887\n",
            "[25/32] [1200/2893] 0.109154\n",
            "[25/32] [1300/2893] 0.066118\n",
            "[25/32] [1400/2893] 0.092744\n",
            "[25/32] [1500/2893] 0.142681\n",
            "[25/32] [1600/2893] 0.117046\n",
            "[25/32] [1700/2893] 0.100975\n",
            "[25/32] [1800/2893] 0.108086\n",
            "[25/32] [1900/2893] 0.102611\n",
            "[25/32] [2000/2893] 0.079037\n",
            "[25/32] [2100/2893] 0.110520\n",
            "[25/32] [2200/2893] 0.126791\n",
            "[25/32] [2300/2893] 0.136523\n",
            "[25/32] [2400/2893] 0.086819\n",
            "[25/32] [2500/2893] 0.133386\n",
            "[25/32] [2600/2893] 0.125004\n",
            "[25/32] [2700/2893] 0.205167\n",
            "[25/32] [2800/2893] 0.053556\n",
            "100%|██████████| 621/621 [01:50<00:00,  5.61it/s]\n",
            "{'joint_goal_accuracy': 0.8480273752012882, 'turn_slot_accuracy': 0.9956521739130476, 'turn_slot_f1': 0.9781248637865184}\n",
            "joint_goal_accuracy: 0.8480273752012882\n",
            "turn_slot_accuracy: 0.9956521739130476\n",
            "turn_slot_f1: 0.9781248637865184\n",
            "[26/32] [0/2893] 0.000153\n",
            "[26/32] [100/2893] 0.000730\n",
            "[26/32] [200/2893] 0.000904\n",
            "[26/32] [300/2893] 0.000795\n",
            "[26/32] [400/2893] 0.000074\n",
            "[26/32] [500/2893] 0.002200\n",
            "[26/32] [600/2893] 0.000034\n",
            "[26/32] [700/2893] 0.000218\n",
            "[26/32] [800/2893] 0.000393\n",
            "[26/32] [900/2893] 0.000162\n",
            "[26/32] [1000/2893] 0.000219\n",
            "[26/32] [1100/2893] 0.000463\n",
            "[26/32] [1200/2893] 0.000087\n",
            "[26/32] [1300/2893] 0.000707\n",
            "[26/32] [1400/2893] 0.001370\n",
            "[26/32] [1500/2893] 0.000581\n",
            "[26/32] [1600/2893] 0.001032\n",
            "[26/32] [1700/2893] 0.000177\n",
            "[26/32] [1800/2893] 0.000511\n",
            "[26/32] [1900/2893] 0.000362\n",
            "[26/32] [2000/2893] 0.000696\n",
            "[26/32] [2100/2893] 0.000834\n",
            "[26/32] [2200/2893] 0.000964\n",
            "[26/32] [2300/2893] 0.006810\n",
            "[26/32] [2400/2893] 0.000483\n",
            "[26/32] [2500/2893] 0.001777\n",
            "[26/32] [2600/2893] 0.001470\n",
            "[26/32] [2700/2893] 0.004921\n",
            "[26/32] [2800/2893] 0.002443\n",
            "[26/32] [0/2893] 0.149288\n",
            "[26/32] [100/2893] 0.128935\n",
            "[26/32] [200/2893] 0.080625\n",
            "[26/32] [300/2893] 0.079153\n",
            "[26/32] [400/2893] 0.081551\n",
            "[26/32] [500/2893] 0.115181\n",
            "[26/32] [600/2893] 0.166795\n",
            "[26/32] [700/2893] 0.124807\n",
            "[26/32] [800/2893] 0.143361\n",
            "[26/32] [900/2893] 0.076415\n",
            "[26/32] [1000/2893] 0.194681\n",
            "[26/32] [1100/2893] 0.096334\n",
            "[26/32] [1200/2893] 0.155782\n",
            "[26/32] [1300/2893] 0.095531\n",
            "[26/32] [1400/2893] 0.146638\n",
            "[26/32] [1500/2893] 0.073881\n",
            "[26/32] [1600/2893] 0.124420\n",
            "[26/32] [1700/2893] 0.188285\n",
            "[26/32] [1800/2893] 0.128748\n",
            "[26/32] [1900/2893] 0.117831\n",
            "[26/32] [2000/2893] 0.154615\n",
            "[26/32] [2100/2893] 0.095322\n",
            "[26/32] [2200/2893] 0.156026\n",
            "[26/32] [2300/2893] 0.121051\n",
            "[26/32] [2400/2893] 0.166287\n",
            "[26/32] [2500/2893] 0.061398\n",
            "[26/32] [2600/2893] 0.163309\n",
            "[26/32] [2700/2893] 0.226663\n",
            "[26/32] [2800/2893] 0.102544\n",
            "100%|██████████| 621/621 [01:49<00:00,  5.70it/s]\n",
            "{'joint_goal_accuracy': 0.8488325281803543, 'turn_slot_accuracy': 0.9956655931293654, 'turn_slot_f1': 0.9776987769899879}\n",
            "joint_goal_accuracy: 0.8488325281803543\n",
            "turn_slot_accuracy: 0.9956655931293654\n",
            "turn_slot_f1: 0.9776987769899879\n",
            "Update Best checkpoint!\n",
            "Best checkpoint saved at ./checkpoint/model_best.bin\n",
            "[27/32] [0/2893] 0.000119\n",
            "[27/32] [100/2893] 0.001151\n",
            "[27/32] [200/2893] 0.000435\n",
            "[27/32] [300/2893] 0.000256\n",
            "[27/32] [400/2893] 0.001454\n",
            "[27/32] [500/2893] 0.000242\n",
            "[27/32] [600/2893] 0.000593\n",
            "[27/32] [700/2893] 0.002884\n",
            "[27/32] [800/2893] 0.000922\n",
            "[27/32] [900/2893] 0.000353\n",
            "[27/32] [1000/2893] 0.002334\n",
            "[27/32] [1100/2893] 0.000124\n",
            "[27/32] [1200/2893] 0.000956\n",
            "[27/32] [1300/2893] 0.000342\n",
            "[27/32] [1400/2893] 0.001650\n",
            "[27/32] [1500/2893] 0.000421\n",
            "[27/32] [1600/2893] 0.000172\n",
            "[27/32] [1700/2893] 0.001173\n",
            "[27/32] [1800/2893] 0.000841\n",
            "[27/32] [1900/2893] 0.000279\n",
            "[27/32] [2000/2893] 0.001037\n",
            "[27/32] [2100/2893] 0.003736\n",
            "[27/32] [2200/2893] 0.000305\n",
            "[27/32] [2300/2893] 0.016491\n",
            "[27/32] [2400/2893] 0.000749\n",
            "[27/32] [2500/2893] 0.000478\n",
            "[27/32] [2600/2893] 0.000054\n",
            "[27/32] [2700/2893] 0.000726\n",
            "[27/32] [2800/2893] 0.001248\n",
            "[27/32] [0/2893] 0.175854\n",
            "[27/32] [100/2893] 0.142199\n",
            "[27/32] [200/2893] 0.130128\n",
            "[27/32] [300/2893] 0.159582\n",
            "[27/32] [400/2893] 0.196694\n",
            "[27/32] [500/2893] 0.075321\n",
            "[27/32] [600/2893] 0.174113\n",
            "[27/32] [700/2893] 0.094760\n",
            "[27/32] [800/2893] 0.182098\n",
            "[27/32] [900/2893] 0.147015\n",
            "[27/32] [1000/2893] 0.097143\n",
            "[27/32] [1100/2893] 0.080567\n",
            "[27/32] [1200/2893] 0.120169\n",
            "[27/32] [1300/2893] 0.212610\n",
            "[27/32] [1400/2893] 0.068434\n",
            "[27/32] [1500/2893] 0.131294\n",
            "[27/32] [1600/2893] 0.071197\n",
            "[27/32] [1700/2893] 0.116869\n",
            "[27/32] [1800/2893] 0.110068\n",
            "[27/32] [1900/2893] 0.085140\n",
            "[27/32] [2000/2893] 0.146150\n",
            "[27/32] [2100/2893] 0.193128\n",
            "[27/32] [2200/2893] 0.074643\n",
            "[27/32] [2300/2893] 0.071704\n",
            "[27/32] [2400/2893] 0.133240\n",
            "[27/32] [2500/2893] 0.178665\n",
            "[27/32] [2600/2893] 0.190495\n",
            "[27/32] [2700/2893] 0.153987\n",
            "[27/32] [2800/2893] 0.098706\n",
            "100%|██████████| 621/621 [01:51<00:00,  5.57it/s]\n",
            "{'joint_goal_accuracy': 0.8516505636070854, 'turn_slot_accuracy': 0.9957729468599068, 'turn_slot_f1': 0.9780268335317774}\n",
            "joint_goal_accuracy: 0.8516505636070854\n",
            "turn_slot_accuracy: 0.9957729468599068\n",
            "turn_slot_f1: 0.9780268335317774\n",
            "Update Best checkpoint!\n",
            "Best checkpoint saved at ./checkpoint/model_best.bin\n",
            "[28/32] [0/2893] 0.000072\n",
            "[28/32] [100/2893] 0.000376\n",
            "[28/32] [200/2893] 0.000107\n",
            "[28/32] [300/2893] 0.000154\n",
            "[28/32] [400/2893] 0.002918\n",
            "[28/32] [500/2893] 0.000205\n",
            "[28/32] [600/2893] 0.000439\n",
            "[28/32] [700/2893] 0.000762\n",
            "[28/32] [800/2893] 0.000071\n",
            "[28/32] [900/2893] 0.000785\n",
            "[28/32] [1000/2893] 0.000169\n",
            "[28/32] [1100/2893] 0.001696\n",
            "[28/32] [1200/2893] 0.000785\n",
            "[28/32] [1300/2893] 0.012486\n",
            "[28/32] [1400/2893] 0.000186\n",
            "[28/32] [1500/2893] 0.000539\n",
            "[28/32] [1600/2893] 0.001299\n",
            "[28/32] [1700/2893] 0.000992\n",
            "[28/32] [1800/2893] 0.001020\n",
            "[28/32] [1900/2893] 0.002290\n",
            "[28/32] [2000/2893] 0.000742\n",
            "[28/32] [2100/2893] 0.000142\n",
            "[28/32] [2200/2893] 0.000195\n",
            "[28/32] [2300/2893] 0.000532\n",
            "[28/32] [2400/2893] 0.000074\n",
            "[28/32] [2500/2893] 0.000653\n",
            "[28/32] [2600/2893] 0.000126\n",
            "[28/32] [2700/2893] 0.000324\n",
            "[28/32] [2800/2893] 0.000477\n",
            "[28/32] [0/2893] 0.167935\n",
            "[28/32] [100/2893] 0.058708\n",
            "[28/32] [200/2893] 0.073141\n",
            "[28/32] [300/2893] 0.107361\n",
            "[28/32] [400/2893] 0.115321\n",
            "[28/32] [500/2893] 0.086825\n",
            "[28/32] [600/2893] 0.177706\n",
            "[28/32] [700/2893] 0.140322\n",
            "[28/32] [800/2893] 0.093110\n",
            "[28/32] [900/2893] 0.056753\n",
            "[28/32] [1000/2893] 0.107663\n",
            "[28/32] [1100/2893] 0.124218\n",
            "[28/32] [1200/2893] 0.199176\n",
            "[28/32] [1300/2893] 0.099391\n",
            "[28/32] [1400/2893] 0.177692\n",
            "[28/32] [1500/2893] 0.116622\n",
            "[28/32] [1600/2893] 0.073021\n",
            "[28/32] [1700/2893] 0.089562\n",
            "[28/32] [1800/2893] 0.116939\n",
            "[28/32] [1900/2893] 0.099905\n",
            "[28/32] [2000/2893] 0.040921\n",
            "[28/32] [2100/2893] 0.042865\n",
            "[28/32] [2200/2893] 0.142166\n",
            "[28/32] [2300/2893] 0.047772\n",
            "[28/32] [2400/2893] 0.174250\n",
            "[28/32] [2500/2893] 0.103767\n",
            "[28/32] [2600/2893] 0.064534\n",
            "[28/32] [2700/2893] 0.153823\n",
            "[28/32] [2800/2893] 0.123852\n",
            "100%|██████████| 621/621 [01:52<00:00,  5.52it/s]\n",
            "{'joint_goal_accuracy': 0.8522544283413849, 'turn_slot_accuracy': 0.9957997852925431, 'turn_slot_f1': 0.9783312839090337}\n",
            "joint_goal_accuracy: 0.8522544283413849\n",
            "turn_slot_accuracy: 0.9957997852925431\n",
            "turn_slot_f1: 0.9783312839090337\n",
            "Update Best checkpoint!\n",
            "Best checkpoint saved at ./checkpoint/model_best.bin\n",
            "[29/32] [0/2893] 0.000504\n",
            "[29/32] [100/2893] 0.000569\n",
            "[29/32] [200/2893] 0.006801\n",
            "[29/32] [300/2893] 0.000775\n",
            "[29/32] [400/2893] 0.000505\n",
            "[29/32] [500/2893] 0.000688\n",
            "[29/32] [600/2893] 0.001223\n",
            "[29/32] [700/2893] 0.000234\n",
            "[29/32] [800/2893] 0.001988\n",
            "[29/32] [900/2893] 0.000055\n",
            "[29/32] [1000/2893] 0.000652\n",
            "[29/32] [1100/2893] 0.000186\n",
            "[29/32] [1200/2893] 0.000242\n",
            "[29/32] [1300/2893] 0.000540\n",
            "[29/32] [1400/2893] 0.000022\n",
            "[29/32] [1500/2893] 0.001382\n",
            "[29/32] [1600/2893] 0.000606\n",
            "[29/32] [1700/2893] 0.000312\n",
            "[29/32] [1800/2893] 0.000235\n",
            "[29/32] [1900/2893] 0.000889\n",
            "[29/32] [2000/2893] 0.000149\n",
            "[29/32] [2100/2893] 0.000208\n",
            "[29/32] [2200/2893] 0.000204\n",
            "[29/32] [2300/2893] 0.000102\n",
            "[29/32] [2400/2893] 0.000228\n",
            "[29/32] [2500/2893] 0.001040\n",
            "[29/32] [2600/2893] 0.000317\n",
            "[29/32] [2700/2893] 0.000165\n",
            "[29/32] [2800/2893] 0.000955\n",
            "[29/32] [0/2893] 0.077087\n",
            "[29/32] [100/2893] 0.172734\n",
            "[29/32] [200/2893] 0.148652\n",
            "[29/32] [300/2893] 0.171743\n",
            "[29/32] [400/2893] 0.063010\n",
            "[29/32] [500/2893] 0.223503\n",
            "[29/32] [600/2893] 0.082477\n",
            "[29/32] [700/2893] 0.090126\n",
            "[29/32] [800/2893] 0.107319\n",
            "[29/32] [900/2893] 0.084176\n",
            "[29/32] [1000/2893] 0.091066\n",
            "[29/32] [1100/2893] 0.083283\n",
            "[29/32] [1200/2893] 0.087886\n",
            "[29/32] [1300/2893] 0.116853\n",
            "[29/32] [1400/2893] 0.148064\n",
            "[29/32] [1500/2893] 0.106892\n",
            "[29/32] [1600/2893] 0.102551\n",
            "[29/32] [1700/2893] 0.063552\n",
            "[29/32] [1800/2893] 0.126177\n",
            "[29/32] [1900/2893] 0.089271\n",
            "[29/32] [2000/2893] 0.091563\n",
            "[29/32] [2100/2893] 0.143029\n",
            "[29/32] [2200/2893] 0.096504\n",
            "[29/32] [2300/2893] 0.122339\n",
            "[29/32] [2400/2893] 0.120628\n",
            "[29/32] [2500/2893] 0.095398\n",
            "[29/32] [2600/2893] 0.161089\n",
            "[29/32] [2700/2893] 0.079558\n",
            "[29/32] [2800/2893] 0.104622\n",
            "100%|██████████| 621/621 [01:51<00:00,  5.57it/s]\n",
            "{'joint_goal_accuracy': 0.8542673107890499, 'turn_slot_accuracy': 0.995866881374132, 'turn_slot_f1': 0.9787193968667558}\n",
            "joint_goal_accuracy: 0.8542673107890499\n",
            "turn_slot_accuracy: 0.995866881374132\n",
            "turn_slot_f1: 0.9787193968667558\n",
            "Update Best checkpoint!\n",
            "Best checkpoint saved at ./checkpoint/model_best.bin\n",
            "[30/32] [0/2893] 0.011952\n",
            "[30/32] [100/2893] 0.001839\n",
            "[30/32] [200/2893] 0.000060\n",
            "[30/32] [300/2893] 0.000889\n",
            "[30/32] [400/2893] 0.000245\n",
            "[30/32] [500/2893] 0.000458\n",
            "[30/32] [600/2893] 0.000569\n",
            "[30/32] [700/2893] 0.000040\n",
            "[30/32] [800/2893] 0.001131\n",
            "[30/32] [900/2893] 0.000023\n",
            "[30/32] [1000/2893] 0.000609\n",
            "[30/32] [1100/2893] 0.000770\n",
            "[30/32] [1200/2893] 0.000106\n",
            "[30/32] [1300/2893] 0.000269\n",
            "[30/32] [1400/2893] 0.000922\n",
            "[30/32] [1500/2893] 0.000271\n",
            "[30/32] [1600/2893] 0.000725\n",
            "[30/32] [1700/2893] 0.000025\n",
            "[30/32] [1800/2893] 0.003131\n",
            "[30/32] [1900/2893] 0.001655\n",
            "[30/32] [2000/2893] 0.000101\n",
            "[30/32] [2100/2893] 0.001164\n",
            "[30/32] [2200/2893] 0.002588\n",
            "[30/32] [2300/2893] 0.000209\n",
            "[30/32] [2400/2893] 0.000422\n",
            "[30/32] [2500/2893] 0.000166\n",
            "[30/32] [2600/2893] 0.004557\n",
            "[30/32] [2700/2893] 0.000452\n",
            "[30/32] [2800/2893] 0.000060\n",
            "[30/32] [0/2893] 0.116077\n",
            "[30/32] [100/2893] 0.108417\n",
            "[30/32] [200/2893] 0.124680\n",
            "[30/32] [300/2893] 0.085151\n",
            "[30/32] [400/2893] 0.105388\n",
            "[30/32] [500/2893] 0.154715\n",
            "[30/32] [600/2893] 0.086808\n",
            "[30/32] [700/2893] 0.136227\n",
            "[30/32] [800/2893] 0.133457\n",
            "[30/32] [900/2893] 0.092445\n",
            "[30/32] [1000/2893] 0.106331\n",
            "[30/32] [1100/2893] 0.206478\n",
            "[30/32] [1200/2893] 0.130813\n",
            "[30/32] [1300/2893] 0.084578\n",
            "[30/32] [1400/2893] 0.073626\n",
            "[30/32] [1500/2893] 0.094315\n",
            "[30/32] [1600/2893] 0.111208\n",
            "[30/32] [1700/2893] 0.112483\n",
            "[30/32] [1800/2893] 0.162189\n",
            "[30/32] [1900/2893] 0.083164\n",
            "[30/32] [2000/2893] 0.113279\n",
            "[30/32] [2100/2893] 0.109066\n",
            "[30/32] [2200/2893] 0.046742\n",
            "[30/32] [2300/2893] 0.102754\n",
            "[30/32] [2400/2893] 0.214701\n",
            "[30/32] [2500/2893] 0.065823\n",
            "[30/32] [2600/2893] 0.123724\n",
            "[30/32] [2700/2893] 0.068999\n",
            "[30/32] [2800/2893] 0.123450\n",
            "100%|██████████| 621/621 [01:51<00:00,  5.58it/s]\n",
            "{'joint_goal_accuracy': 0.856280193236715, 'turn_slot_accuracy': 0.9959384505278276, 'turn_slot_f1': 0.979236651470845}\n",
            "joint_goal_accuracy: 0.856280193236715\n",
            "turn_slot_accuracy: 0.9959384505278276\n",
            "turn_slot_f1: 0.979236651470845\n",
            "Update Best checkpoint!\n",
            "Best checkpoint saved at ./checkpoint/model_best.bin\n",
            "[31/32] [0/2893] 0.001978\n",
            "[31/32] [100/2893] 0.000897\n",
            "[31/32] [200/2893] 0.001403\n",
            "[31/32] [300/2893] 0.003234\n",
            "[31/32] [400/2893] 0.000143\n",
            "[31/32] [500/2893] 0.000381\n",
            "[31/32] [600/2893] 0.008897\n",
            "[31/32] [700/2893] 0.000027\n",
            "[31/32] [800/2893] 0.003864\n",
            "[31/32] [900/2893] 0.001313\n",
            "[31/32] [1000/2893] 0.005103\n",
            "[31/32] [1100/2893] 0.002159\n",
            "[31/32] [1200/2893] 0.001007\n",
            "[31/32] [1300/2893] 0.000278\n",
            "[31/32] [1400/2893] 0.000161\n",
            "[31/32] [1500/2893] 0.000108\n",
            "[31/32] [1600/2893] 0.000133\n",
            "[31/32] [1700/2893] 0.000047\n",
            "[31/32] [1800/2893] 0.000024\n",
            "[31/32] [1900/2893] 0.000186\n",
            "[31/32] [2000/2893] 0.002376\n",
            "[31/32] [2100/2893] 0.000173\n",
            "[31/32] [2200/2893] 0.000930\n",
            "[31/32] [2300/2893] 0.000098\n",
            "[31/32] [2400/2893] 0.000028\n",
            "[31/32] [2500/2893] 0.000472\n",
            "[31/32] [2600/2893] 0.002797\n",
            "[31/32] [2700/2893] 0.001018\n",
            "[31/32] [2800/2893] 0.002509\n",
            "[31/32] [0/2893] 0.115515\n",
            "[31/32] [100/2893] 0.047926\n",
            "[31/32] [200/2893] 0.120216\n",
            "[31/32] [300/2893] 0.157145\n",
            "[31/32] [400/2893] 0.112724\n",
            "[31/32] [500/2893] 0.102747\n",
            "[31/32] [600/2893] 0.113919\n",
            "[31/32] [700/2893] 0.070627\n",
            "[31/32] [800/2893] 0.062054\n",
            "[31/32] [900/2893] 0.171470\n",
            "[31/32] [1000/2893] 0.158033\n",
            "[31/32] [1100/2893] 0.151876\n",
            "[31/32] [1200/2893] 0.104897\n",
            "[31/32] [1300/2893] 0.093841\n",
            "[31/32] [1400/2893] 0.114115\n",
            "[31/32] [1500/2893] 0.130075\n",
            "[31/32] [1600/2893] 0.123737\n",
            "[31/32] [1700/2893] 0.144818\n",
            "[31/32] [1800/2893] 0.118353\n",
            "[31/32] [1900/2893] 0.167784\n",
            "[31/32] [2000/2893] 0.121928\n",
            "[31/32] [2100/2893] 0.071122\n",
            "[31/32] [2200/2893] 0.114023\n",
            "[31/32] [2300/2893] 0.171231\n",
            "[31/32] [2400/2893] 0.096684\n",
            "[31/32] [2500/2893] 0.034139\n",
            "[31/32] [2600/2893] 0.063927\n",
            "[31/32] [2700/2893] 0.056050\n",
            "[31/32] [2800/2893] 0.130701\n",
            "100%|██████████| 621/621 [01:49<00:00,  5.67it/s]\n",
            "{'joint_goal_accuracy': 0.855475040257649, 'turn_slot_accuracy': 0.9958758275183444, 'turn_slot_f1': 0.9788800358402852}\n",
            "joint_goal_accuracy: 0.855475040257649\n",
            "turn_slot_accuracy: 0.9958758275183444\n",
            "turn_slot_f1: 0.9788800358402852\n",
            "[32/32] [0/2893] 0.000097\n",
            "[32/32] [100/2893] 0.000964\n",
            "[32/32] [200/2893] 0.000152\n",
            "[32/32] [300/2893] 0.001333\n",
            "[32/32] [400/2893] 0.000600\n",
            "[32/32] [500/2893] 0.000103\n",
            "[32/32] [600/2893] 0.003315\n",
            "[32/32] [700/2893] 0.000513\n",
            "[32/32] [800/2893] 0.000825\n",
            "[32/32] [900/2893] 0.000180\n",
            "[32/32] [1000/2893] 0.001685\n",
            "[32/32] [1100/2893] 0.000081\n",
            "[32/32] [1200/2893] 0.001434\n",
            "[32/32] [1300/2893] 0.000297\n",
            "[32/32] [1400/2893] 0.002179\n",
            "[32/32] [1500/2893] 0.004170\n",
            "[32/32] [1600/2893] 0.000375\n",
            "[32/32] [1700/2893] 0.000048\n",
            "[32/32] [1800/2893] 0.000193\n",
            "[32/32] [1900/2893] 0.000093\n",
            "[32/32] [2000/2893] 0.000564\n",
            "[32/32] [2100/2893] 0.000635\n",
            "[32/32] [2200/2893] 0.001209\n",
            "[32/32] [2300/2893] 0.000608\n",
            "[32/32] [2400/2893] 0.000022\n",
            "[32/32] [2500/2893] 0.000681\n",
            "[32/32] [2600/2893] 0.000047\n",
            "[32/32] [2700/2893] 0.000273\n",
            "[32/32] [2800/2893] 0.001021\n",
            "[32/32] [0/2893] 0.089131\n",
            "[32/32] [100/2893] 0.077214\n",
            "[32/32] [200/2893] 0.130998\n",
            "[32/32] [300/2893] 0.057079\n",
            "[32/32] [400/2893] 0.076939\n",
            "[32/32] [500/2893] 0.049147\n",
            "[32/32] [600/2893] 0.116015\n",
            "[32/32] [700/2893] 0.112155\n",
            "[32/32] [800/2893] 0.171657\n",
            "[32/32] [900/2893] 0.093899\n",
            "[32/32] [1000/2893] 0.051677\n",
            "[32/32] [1100/2893] 0.120035\n",
            "[32/32] [1200/2893] 0.099778\n",
            "[32/32] [1300/2893] 0.070519\n",
            "[32/32] [1400/2893] 0.076726\n",
            "[32/32] [1500/2893] 0.083005\n",
            "[32/32] [1600/2893] 0.057256\n",
            "[32/32] [1700/2893] 0.116777\n",
            "[32/32] [1800/2893] 0.116973\n",
            "[32/32] [1900/2893] 0.082387\n",
            "[32/32] [2000/2893] 0.111634\n",
            "[32/32] [2100/2893] 0.112389\n",
            "[32/32] [2200/2893] 0.177306\n",
            "[32/32] [2300/2893] 0.059115\n",
            "[32/32] [2400/2893] 0.118245\n",
            "[32/32] [2500/2893] 0.065121\n",
            "[32/32] [2600/2893] 0.200315\n",
            "[32/32] [2700/2893] 0.059804\n",
            "[32/32] [2800/2893] 0.112226\n",
            "100%|██████████| 621/621 [01:51<00:00,  5.59it/s]\n",
            "{'joint_goal_accuracy': 0.855475040257649, 'turn_slot_accuracy': 0.9958713544462384, 'turn_slot_f1': 0.9788604598602526}\n",
            "joint_goal_accuracy: 0.855475040257649\n",
            "turn_slot_accuracy: 0.9958713544462384\n",
            "turn_slot_f1: 0.9788604598602526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48021e08"
      },
      "source": [
        "## Inference"
      ],
      "id": "48021e08"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b02cb1fb",
        "outputId": "762054ee-ec22-496b-9bcb-a9176f05273f"
      },
      "source": [
        "eval_data = json.load(open(f\"/opt/ml/input/data/eval_dataset/eval_dials.json\", \"r\"))\n",
        "\n",
        "eval_examples = get_examples_from_dialogues(\n",
        "    eval_data, user_first=False, dialogue_level=False\n",
        ")\n",
        "\n",
        "# Extracting Featrues\n",
        "eval_features = processor.convert_examples_to_features(eval_examples)\n",
        "eval_data = WOSDataset(eval_features)\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_loader = DataLoader(\n",
        "    eval_data,\n",
        "    batch_size=8,\n",
        "    sampler=eval_sampler,\n",
        "    collate_fn=processor.collate_fn,\n",
        ")"
      ],
      "id": "b02cb1fb",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:01<00:00, 1945.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"./checkpoint/model_best.bin\"))\n",
        "\n",
        "# Best 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ef9937",
        "outputId": "070878e0-5c13-4268-e4a4-ca32254334b7"
      },
      "source": [
        "predictions = inference(model, eval_loader, processor, device)"
      ],
      "id": "b2ef9937",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1847/1847 [06:52<00:00,  4.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03a7163b"
      },
      "source": [
        "json.dump(predictions, open('predictions.csv', 'w'), indent=2, ensure_ascii=False)"
      ],
      "id": "03a7163b",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62efcf98"
      },
      "source": [],
      "id": "62efcf98",
      "execution_count": null,
      "outputs": []
    }
  ]
}